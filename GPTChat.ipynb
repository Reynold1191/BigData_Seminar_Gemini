{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 0.28.0\n",
      "Summary: Python client library for the OpenAI API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: OpenAI\n",
      "Author-email: support@openai.com\n",
      "License: \n",
      "Location: C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\n",
      "Requires: aiohttp, requests, tqdm\n",
      "Required-by: gpt-index, llama-index, revChatGPT\n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44642eb905124fe68bbbcd75ba8a9cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "themes/theme_schema@1.0.0.json:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tttoa\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.8.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "IMPORTANT: You are using gradio version 4.8.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Temp\\ipykernel_12260\\1517731557.py\", line 86, in classification\n",
      "    response = get_response(messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Temp\\ipykernel_12260\\1517731557.py\", line 14, in get_response\n",
      "    completion = openai.Completion.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: Unrecognized request argument supplied: messages\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 456, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1522, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\blocks.py\", line 1144, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\utils.py\", line 674, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Temp\\ipykernel_12260\\1517731557.py\", line 86, in classification\n",
      "    response = get_response(messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Temp\\ipykernel_12260\\1517731557.py\", line 14, in get_response\n",
      "    completion = openai.Completion.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_resources\\completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "                           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\api_requestor.py\", line 765, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.InvalidRequestError: Unrecognized request argument supplied: messages\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 501, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\tttoa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gradio\\queueing.py\", line 465, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# from flask import Flask, jsonify, render_template, request, session\n",
    "import gradio as gr\n",
    "openai.api_key=\"\"\n",
    "\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "def format_message(role, content):\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def get_response(messages):\n",
    "    completion = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = completion.choices[0].message.content\n",
    "    return content\n",
    "\n",
    "user_inputs = [\n",
    "    \"I already know this\",\n",
    "    \"I love this\",\n",
    "    \"The food at this restaurant was out of this world!\",\n",
    "    \"Vô dụng vl\",\n",
    "    \"Cảm ơn bạn nhiều nhá\",\n",
    "    \"Bạn có thêm nói rõ hơn được không\",\n",
    "    \"Cái này mình biết trước rồi\"\n",
    "]\n",
    "\n",
    "class_inst = f\"\"\"\n",
    "Your task is to classify the comment into one of the following categories:\n",
    "Positive, Negative\n",
    "\n",
    "Positive comments are:\n",
    "- comments provide useful information \n",
    "\n",
    "Negative comments are:\n",
    "- when the comments offend any individual in some types of way\n",
    "- comments express negative attitude\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "responses = []\n",
    "\n",
    "# for user_input in user_inputs:\n",
    "#     messages = [format_message(\"system\", class_inst),\n",
    "#                 format_message(\"user\", user_input)]\n",
    "#     response = get_response(messages)\n",
    "#     responses.append(response)\n",
    "#     print(user_input)\n",
    "#     print(response)\n",
    "\n",
    "script = \"\"\"\n",
    "function createGradioWelcome() {\n",
    "    // Create a container div\n",
    "    var container = document.createElement('div');\n",
    "    container.id = 'gradio-welcome';\n",
    "    \n",
    "    // Set styles for the container\n",
    "    container.style.fontSize = '2em';\n",
    "    container.style.fontWeight = 'bold';\n",
    "    container.style.textAlign = 'center';\n",
    "    container.style.marginBottom = '20px';\n",
    "    \n",
    "    // Create a div for the welcome text\n",
    "    var welcomeText = document.createElement('div');\n",
    "    welcomeText.innerText = \"Comment Classification\";\n",
    "    \n",
    "    // Append the welcome text to the container\n",
    "    container.appendChild(welcomeText);\n",
    "    \n",
    "    // Find the Gradio container and insert the welcome message at the top\n",
    "    var gradioContainer = document.querySelector('.gradio-container');\n",
    "    gradioContainer.insertBefore(container, gradioContainer.firstChild);\n",
    "    \n",
    "    return 'Welcome message displayed';\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def classification(text):\n",
    "    messages = [format_message(\"system\", class_inst),\n",
    "                format_message(\"user\", text)]\n",
    "    response = get_response(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "with gr.Blocks(theme='JohnSmith9982/small_and_pretty', js = script) as demo:\n",
    "    gr.Interface(\n",
    "    classification,\n",
    "    inputs=[gr.Textbox(label=\"Enter your text\", lines=3)],\n",
    "    outputs=[gr.Textbox(label=\"Classification\", lines=3)],\n",
    "    allow_flagging=\"never\",\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chào bạn! Hãy bắt đầu trò chuyện với ChatGPT.\n",
      "ChatGPT: Hello! How can I assist you today?\n",
      "ChatGPT: Đó là câu chào hỏi thông thường khi bạn muốn bắt đầu một cuộc trò chuyện. Bạn cần giúp đỡ hoặc thông tin gì không?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Thay thế 'your-api-key' bằng khóa API của bạn từ OpenAI\n",
    "openai.api_key = ''\n",
    "\n",
    "def chat_with_gpt(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Hoặc mô hình chat khác bạn đang sử dụng\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def continuous_chat():\n",
    "    print(\"Chào bạn! Hãy bắt đầu trò chuyện với ChatGPT.\")\n",
    "    messages = []\n",
    "    while True:\n",
    "        user_input = input(\"Bạn: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"ChatGPT: Tạm biệt! Hẹn gặp lại.\")\n",
    "            break\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        response = chat_with_gpt(messages)\n",
    "        print(f\"ChatGPT: {response}\")\n",
    "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    continuous_chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
